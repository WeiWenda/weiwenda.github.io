---
layout: post
category:
tags: []
---
> 纸上得来终觉浅，绝知此事要躬行，请重点阅读或者亲自尝试每个主题末尾的实验验证。

- 从OOM、running beyond physical memory limits谈起

    > 分区数非动态给出以及unbounded shuffle的使用导致程序无法适应规模逐渐增加的数据

    - Executor内存配置

        > 推荐将spark.executor.cores调大至2-5，将spark.executor.memory调大至
        `yarn.scheduler.maximum-allocation-mb - spark.yarn.executor.memoryOverhead`，也就是`0.9*yarn.scheduler.maximum-allocation-mb`。同时打开动态调度spark.dynamicAllocation.enabled=true

        - yarn.scheduler.maximum-allocation-mb 限制了Container的最大内存
        - yarn.scheduler.maximum-allocation-vcores 限制了Container的最大虚拟核数
        - yarn.scheduler.minimum-allocation-mb限制了Container的最小内存
        - yarn.scheduler.increment-allocation-mb 对Container的内存占用取整数倍对齐
        - spark.yarn.executor.memoryOverhead，默认为max(384MB, 0.1 * spark.executor.memory)

        ![](http://blog.cloudera.com/wp-content/uploads/2015/03/spark-tuning2-f1.png)

        [实验验证：控制Executor内存](https://www.notion.so/111091ad06a24867a17495bf100a9608)

    - 分区数配置

        > 调整分区数的终极目标是将Task任务大小控制在64MB-1024MB之间，是避免OOM的最直接的方式，因此请勿将repartition(xxx)写死在定时任务中，可以用以下方案替代：

        - 配置spark.default.parallelism，默认totalCores
        - sql情况下配置spark.sql.shuffle.partitions，默认200
        - 先统计再进行rdd.repartition(numPartitions)或***ByKey(func, numPartitions)
            1. 统计RDD的总记录数
            2. 统计RDD单条记录的运行字节数或序列化字节数
            3. numPartitions=总记录数*单条记录运行字节数/执行器可用执行内存 或者
            numPartitions=总记录数*单条记录序列化字节数/期待shuffle read大小(128MB)

        [实验验证：控制shuffle分区](https://www.notion.so/5066c05a210540f89726fec1907a1e28)

    - 使用bounded shuffle

        > 如果你在操作TB量级的表或文件集，而且没有使用DataFrame API，那么请思考是否已经做到以下几点：

        1. shuffle前已经将用不到的字段卸掉
        2. aggregateByKey而不是groupByKey
        3. treeReduce而不是reduce
        4. 聚集结果是统计值，而不是集合，就算是集合，也在map端进行过slice

        [实验验证：使用bounded shuffle避免OOM](https://www.notion.so/52d42b1dd62c420bac7449e4cf58dae8)

    - 增量计算而不是全量计算
    - 参考文献
        1. [https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/](https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/)
        2. [https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/](https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/)
- 硬核问题：如何解决聚集数据倾斜、连接数据倾斜

    > map端或join大表端存在isolated key或null key时，reduce端任务大小失衡，倾斜分区的shuffle read size/records超过中位数的2倍以上，这种情况下分区数无论如何设置多大，都一定会出现拖尾任务或OOM。

    - 加盐聚集

        > 加盐聚集需要与bounded shuffle配合，否则依然无济于事

        1. 选择加盐系数K，2-8，数据倾斜越严重，系数选择越大
        2. 向RDD的记录添加新列，记为salt，使用Random.nextInt(K)随机填充
        3. 将原聚集维度记为okey，使用(okey，salt)作为新的聚集维度，进行第一次聚集
        4. 重新使用okey作为聚集维度，进行第二次聚集

        > 事实上当你清楚的知道isolated key是哪几个时，可以只向这些记录加盐，其他记录的salt列可以用0填充，同时第二次聚集也只需要在isolated key所在的记录中进行

        [实验验证：通过加盐解决聚集数据倾斜](https://www.notion.so/b21fec481be94c499488978b9f93445f)

    - Map-side Join

        > 当你（根据经验或经过统计后）清楚的知道isolated key是哪几个后，Map-side Join将是解决问题的更好方案，它将原本的任务分成倾斜部分和非倾斜部分分别执行，最后union两部分的结果就是原任务的结果。特别注意，不要因为拆分任务麻烦就执念于加资源解决问题，除非这是一个临时任务，一味通过加内存跑通数据倾斜的任务会造成严重的资源浪费。

        1. 假设isolated key是1，那么select A.id from A join B on A.id = B.id 可以分为下面两部分:
            - select A.id from A join B on A.id = B.id where A.id <> 1
            - select A.id from A join B on A.id = B.id where A.id = 1 and B.id = 1
        2. 当Adaptive Execution开启时，小表B一侧的中间结果将足够小而被广播到大表一侧进行Map-side Join，大表在整个过程中不再需要shuffle
        3. 如果Adaptive Execution没有开启，你仍然可以通过cache table实现Map-side Join
        4. 如果isolated key过多，你可以通过加盐快速实现类似Map-side Join的操作：
            1. 大表A添加新列，记为saltA，使用Random.nextInt(K)随机填充
            2. 小表B复制K次，第k次复制时添加的新列统一填充为k-1，最终saltB列的值将从0到K-1变化
            3. 执行select A.id from A join B on A.id = B.id and A.saltA = B.saltB，这种情况下大表虽然进行了shuffle，但数据倾斜被消除了

        [实验验证：通过Map-side Join解决连接数据倾斜](https://www.notion.so/f1230da3c01844f5a982c59bb93e046f)

    - 参考文献
        1. [https://stackoverflow.com/questions/40373577/skewed-dataset-join-in-spark](https://stackoverflow.com/questions/40373577/skewed-dataset-join-in-spark)
        2. [https://datarus.wordpress.com/2015/05/04/fighting-the-skew-in-spark/](https://datarus.wordpress.com/2015/05/04/fighting-the-skew-in-spark/)
        3. [http://www.jasongj.com/spark/skew/](http://www.jasongj.com/spark/skew/)
- 初始分区数控制

    > 由于RDD默认分区数spark.default.parallelism、DataFrame默认分区数*spark.sql.shuffle.partitions只对shuffle生效，导致第一个map任务大小不合理*

    - SparkContext.textFIle(path, numSplits=2)

            任务大小 = minSize, goalSize, blockSize 三者中间值
            
            minSize = max(mapreduce.input.fileinputformat.split.minsize, mapred.min.split.size)
            mapreduce.input.fileinputformat.split.minsize，默认为0，配置方法 --conf spark.hadoop.mapreduce.input.fileinputformat.split.minsize
            goalSize = min(job.getLong("mapred.max.split.size", Long.MAX_VALUE),totalSize / (numSplits == 0 ? 1 : numSplits))
            mapred.max.split.size和mapred.min.split.size配置在hive-site.xml中，不使用sql时是不会加载该配置文件的(sharedState被标记为lazy)，此处为512MB，默认分别为Long.MAX_VALUE和1
            <property>
              <name>mapred.min.split.size</name>
              <value>512000000</value>
            </property>
            <property>
              <name>mapred.max.split.size</name>
              <value>512000000</value>
            </property>
            blockSize由hdfs-site.xml中的配置决定，此处256MB
            <property>
              <name>dfs.block.size</name>
              <value>268435456</value>
            </property>

        注意，源数据是不可切分文件时，如gz文件，上述计算公式失效。

    - Spark.read

            任务大小 = min(spark.sql.files.maxPartitionBytes, max(spark.sql.files.openCostInBytes, totalSize/spark.default.parallelism))
            
            spark.sql.files.maxPartitionBytes，默认为128MB
            spark.sql.files.openCostInBytes，默认为4MB
            spark.default.parallelism，默认为max(totalCores, 2)，spark.dynamicAllocation.enabled为true时，初始totalCores=spark.dynamicAllocation.initialExecutors*spark.executor.cores

    - Spark.sql 计算方法同SparkContext.textFIle，但numSplits=max(mapreduce.job.maps, 2)

    [实验验证：控制初始分区数](https://www.notion.so/234c97e501fa484bb3045c618f1c8622)

    - 参考文献
        1. [https://ittechnews.net/microsoft/official-microsoft-news/how-does-spark-determine-partitions-for-an-rdd/](https://ittechnews.net/microsoft/official-microsoft-news/how-does-spark-determine-partitions-for-an-rdd/)
- 快速debug

    > 为了重现集群运行错误，更换配置后花费hours的时间等待再次报错

    - 开始使用spark shell
    - 开始使用checkpoint

> 处理数据规模问题的步骤，总结起来就是：
1.先改资源 2.后改分区 3.再考虑shuffle限界 4.最后解决倾斜。
上述四个步骤是一个持续优化的过程，不要奢求一步到位。

- 参考文献
    1. [https://www.youtube.com/watch?v=WyfHUNnMutg](https://www.youtube.com/watch?v=WyfHUNnMutg)